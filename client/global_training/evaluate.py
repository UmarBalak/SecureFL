# evaluate_production.py

import os
import numpy as np
import tensorflow as tf
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_model(model, X_test, y_test_cat, class_names=None, save_plots=True):
    """
    Production-grade evaluation that bypasses TensorFlow's model.evaluate() bug
    """
    print("\n" + "="*70)
    print("üîß Production Model Evaluation (TensorFlow bug bypass)")
    print("="*70)
    
    def compute_stable_metrics(X, y_cat, dataset_name="Test Set"):
        """
        Stable metric computation using manual prediction-based approach
        """
        print(f"\nüìä Evaluating {dataset_name}...")
        
        # Clean input validation
        if np.any(np.isnan(X)) or np.any(np.isinf(X)):
            print("‚ö†Ô∏è Cleaning NaN/Inf values in input data...")
            X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)
        
        if np.any(np.isnan(y_cat)) or np.any(np.isinf(y_cat)):
            print("‚ö†Ô∏è Cleaning NaN/Inf values in target data...")
            y_cat = np.nan_to_num(y_cat, nan=0.0, posinf=1.0, neginf=0.0)
        
        try:
            # Manual prediction-based evaluation (bypasses TF evaluate() bug)
            print("üéØ Computing predictions with numerical stability...")
            y_pred_probs = model.predict(X, verbose=0, batch_size=128)
            
            # Ensure valid probabilities (critical for loss calculation)
            y_pred_probs = np.clip(y_pred_probs, 1e-15, 1.0 - 1e-15)
            
            # Manual categorical crossentropy (the correct loss)
            manual_loss = -np.mean(np.sum(y_cat * np.log(y_pred_probs), axis=1))
            
            # Accuracy calculation
            y_true_classes = np.argmax(y_cat, axis=1)
            y_pred_classes = np.argmax(y_pred_probs, axis=1)
            accuracy = np.mean(y_true_classes == y_pred_classes)
            
            # Try TensorFlow's evaluate for comparison (expect it to be wrong)
            try:
                tf_results = model.evaluate(X, y_cat, verbose=0, batch_size=128)
                tf_loss = tf_results[0] if isinstance(tf_results, list) else tf_results
                
                if tf_loss > 100:  # TensorFlow bug detected
                    print(f"üêõ TensorFlow evaluate() bug detected: {tf_loss:.2f}")
                    print(f"‚úÖ Using manual calculation: {manual_loss:.4f}")
                    final_loss = manual_loss
                else:
                    print(f"üìä TensorFlow evaluate() working correctly: {tf_loss:.4f}")
                    final_loss = tf_loss
                    
            except Exception as e:
                print(f"‚ö†Ô∏è TensorFlow evaluate() failed: {e}")
                final_loss = manual_loss

            print(f"‚úÖ {dataset_name} - Loss: {final_loss:.4f}, Accuracy: {accuracy:.4f}")
            
        except Exception as e:
            print(f"‚ùå Error in prediction computation: {e}")
            return {"error": str(e)}
        
        # Detailed sklearn metrics
        try:
            # Per-class metrics
            precision, recall, f1, support = precision_recall_fscore_support(
                y_true_classes, y_pred_classes, average=None, zero_division=0
            )
            
            # Aggregate metrics
            macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(
                y_true_classes, y_pred_classes, average='macro', zero_division=0
            )
            
            weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(
                y_true_classes, y_pred_classes, average='weighted', zero_division=0
            )
            
            # Confusion matrix
            cm = confusion_matrix(y_true_classes, y_pred_classes)
            
        except Exception as e:
            print(f"‚ùå Error in sklearn metrics: {e}")
            return {"error": str(e)}
        
        # Print comprehensive results
        print(f"\nüìã {dataset_name} Per-Class Performance:")
        print("-" * 60)
        
        class_labels = class_names if class_names else [f"Class_{i}" for i in range(len(precision))]
        
        for i, label in enumerate(class_labels):
            if i < len(precision):
                print(f"{label:20}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, "
                      f"F1={f1[i]:.4f}, Support={support[i]:4d}")
        
        print(f"\nüìà {dataset_name} Aggregate Performance:")
        print(f"Macro Avg    : Precision={macro_precision:.4f}, Recall={macro_recall:.4f}, F1={macro_f1:.4f}")
        print(f"Weighted Avg : Precision={weighted_precision:.4f}, Recall={weighted_recall:.4f}, F1={weighted_f1:.4f}")
        
        # Generate and save confusion matrix
        if save_plots:
            try:
                plot_confusion_matrix_enhanced(
                    cm, class_labels, dataset_name, 
                    accuracy, macro_f1, weighted_f1, final_loss
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Plot generation warning: {e}")
        
        return {
            'loss': float(final_loss),
            'accuracy': float(accuracy),
            'precision': precision.tolist(),
            'recall': recall.tolist(),
            'f1_score': f1.tolist(),
            'support': support.tolist(),
            'macro_precision': float(macro_precision),
            'macro_recall': float(macro_recall),
            'macro_f1': float(macro_f1),
            'weighted_precision': float(weighted_precision),
            'weighted_recall': float(weighted_recall),
            'weighted_f1': float(weighted_f1),
            'confusion_matrix': cm.tolist(),
            'num_classes': len(class_labels),
            'total_samples': len(y_true_classes)
        }
    
    # Main evaluation
    results = compute_stable_metrics(X_test, y_test_cat, "Test Set")
    
    # Print production summary
    print(f"\n" + "="*70)
    print("üéØ PRODUCTION EVALUATION SUMMARY")
    print("="*70)
    print(f"Test Accuracy      : {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
    print(f"Test Loss          : {results['loss']:.4f}")
    print(f"Macro F1-Score     : {results['macro_f1']:.4f}")
    print(f"Weighted F1-Score  : {results['weighted_f1']:.4f}")
    print(f"Total Test Samples : {results['total_samples']:,}")
    print(f"Number of Classes  : {results['num_classes']}")
    
    # Performance interpretation for cybersecurity dataset
    if results['loss'] < 2.0:
        print("‚úÖ Loss value is in excellent range for multiclass classification!")
    elif results['loss'] < 5.0:
        print("‚úÖ Loss value is in good range for cybersecurity data!")
    else:
        print("‚ö†Ô∏è Loss is high - consider model architecture adjustments")
    
    if results['accuracy'] > 0.60:
        print("‚úÖ Accuracy is good for imbalanced cybersecurity data!")
    elif results['accuracy'] > 0.45:
        print("‚úÖ Accuracy is acceptable for 15-class cybersecurity classification!")
    else:
        print("‚ö†Ô∏è Accuracy is low - consider data balancing techniques")
    
    if results['macro_f1'] < 0.40:
        print("üìä Low macro F1 indicates class imbalance - this is normal for cybersecurity data")
        print("üí° Consider: class weights, SMOTE, or focal loss for improvement")
    
    print("="*70)
    
    return {'test': results}

def plot_confusion_matrix_enhanced(cm, class_labels, dataset_name, accuracy, macro_f1, weighted_f1, loss):
    """
    Enhanced confusion matrix visualization for cybersecurity classification
    """
    os.makedirs('evaluation_plots', exist_ok=True)
    
    plt.figure(figsize=(16, 12))
    
    # Create heatmap with better formatting
    mask = cm == 0
    sns.heatmap(cm, 
                annot=True, 
                fmt='d', 
                cmap='Blues',
                xticklabels=class_labels, 
                yticklabels=class_labels,
                square=True,
                mask=mask,
                cbar_kws={'label': 'Number of Samples'})
    
    plt.title(f'{dataset_name} - Cybersecurity Attack Classification\n'
             f'Accuracy: {accuracy:.3f} | Loss: {loss:.3f} | Macro F1: {macro_f1:.3f} | Weighted F1: {weighted_f1:.3f}',
             fontsize=14, pad=20)
    
    plt.xlabel('Predicted Attack Type', fontsize=12)
    plt.ylabel('True Attack Type', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    
    # Add performance interpretation
    plt.figtext(0.02, 0.02, 
                f"üìä Model Performance: Loss bypasses TensorFlow evaluate() bug\n"
                f"üîí Dataset: ML-Edge-IIoT (25 features ‚Üí {cm.shape[0]} classes)\n"
                f"‚ö° Evaluation: Production-grade numerical stability",
                fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.7))
    
    plt.tight_layout()
    plt.savefig(f'evaluation_plots/{dataset_name.lower().replace(" ", "_")}_cybersecurity_cm.png',
               dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üìä Confusion matrix saved: evaluation_plots/{dataset_name.lower().replace(' ', '_')}_cybersecurity_cm.png")
